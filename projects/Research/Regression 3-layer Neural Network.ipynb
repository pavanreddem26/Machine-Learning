{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3 -Layer Neural Network for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Relu Output---\n",
    "def relu_output(value):\n",
    "    if(value>=0):\n",
    "        return(value)\n",
    "    else:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_output(value):\n",
    "    if(value>0):\n",
    "        return(value)\n",
    "    elif(value<0):\n",
    "        return(value)\n",
    "    else:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_output(value):\n",
    "    if(value>=(-709)):\n",
    "        a=(1+math.exp(-value))\n",
    "        return(1/a)\n",
    "    else:\n",
    "        value=value%100\n",
    "        a=(1+math.exp(-value))\n",
    "        return(1/a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_diff(value):\n",
    "    if(value>=0):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_diff(value):\n",
    "    if(value>0):\n",
    "        return(1)\n",
    "    elif(value<0):\n",
    "        return(-1)\n",
    "    else:\n",
    "        return (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_diff(value):\n",
    "    if(value>=(-709)):\n",
    "        a=(1 + math.exp(-value))\n",
    "        b=math.exp(-value)\n",
    "        return(b/(a*a))\n",
    "    else:\n",
    "        value=value%10\n",
    "        a=(1 + math.exp(-value))\n",
    "        b=math.exp(-value)\n",
    "        return(b/(a*a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Change the Path Directory when ever you want--\n",
    "path=r\"C:\\Users\\Pavan\\Desktop\\NN testing\\regression\"\n",
    "os.chdir(path)\n",
    "print(\"Path Diectory:\",os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps to follow about the data--\n",
    "## Load the data file into the training Environment  \n",
    "## Make sure that the dataset is completely clean if it is not clean then deal with it\n",
    "## After the data set is completely clean the calculate the total number of rows\n",
    "##This calculated rows is used to train the model that many times to adapt with respect to each and every data point\n",
    "col_names=[\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\",\"MEDV\"]\n",
    "training_data=pd.read_csv(\"housing_train.txt\", sep=\"\\s+\", header=None,names=col_names)\n",
    "training_data  # This gives the total number of the Rows in the Data Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating the label from training data\n",
    "y=training_data[\"MEDV\"]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting the label from the training Data\n",
    "del training_data[\"MEDV\"]\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assuming that the data is completely clean --\n",
    "# Now I am calculating the total numebr of times I had to loop\n",
    "total_length=len(training_data)\n",
    "total_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_columns=len(training_data.columns)\n",
    "total_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_layer_nodes(training_data,Dimension_reduction):\n",
    "    \n",
    "    \n",
    "    if(Dimension_reduction==\"NO\"):\n",
    "        input_nodes=len(training_data.columns)\n",
    "        if(input_nodes==0):\n",
    "            return(1)   #Adding this as the bias node\n",
    "        else:\n",
    "            return(input_nodes) #Make sure to remove the Label(Y) from the training data\n",
    "                                #If you don't want to remove the label(Y) then consider input_nodes-1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_activation(layers):\n",
    "    \n",
    "    print(\"Enter the Activation function for the Hidden Layer\")\n",
    "    print(\"Enter 'relu' for Rectified Linear Unit\")\n",
    "    print(\"Enter 'sigmoid' for Sigmoid Activation Unit\")\n",
    "    print(\"Enter 'linear' for Linear Activation Function\")\n",
    "    \n",
    "    if(layers==3):\n",
    "        print(\"In the 3-layer Neural Network will have only one Hidden Layer and one Activation Function\")\n",
    "        activation_function=input(\"Enter the activation Function in the hidden Layer\")\n",
    "        output_active=activ_function(activation_function)\n",
    "        diff_active=diff_activation(activation_function)\n",
    "        return[output_active,diff_active]\n",
    "    else:\n",
    "                \n",
    "        print(\"You entered the wrong option\")\n",
    "        print('**Please Start Again**')\n",
    "        main_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activ_function(activation_function):\n",
    "    \n",
    "    if(activation_function=='relu'):\n",
    "        \n",
    "        activ_function=relu_output\n",
    "        return(activ_function)\n",
    "    \n",
    "    elif(activation_function=='sigmoid'):\n",
    "        \n",
    "        activ_function=sigmoid_output\n",
    "        return(activ_function)\n",
    "    \n",
    "    elif(activation_function=='linear'):\n",
    "        \n",
    "        activ_function=linear_output\n",
    "        return(activ_function)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print(\"You entered the wrong option\")\n",
    "        print('**Please Start Again**')\n",
    "        main_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_activation(activation_function):\n",
    "    \n",
    "    if(activation_function=='relu'):\n",
    "        \n",
    "        diff_act=relu_diff\n",
    "        return(diff_act)\n",
    "    \n",
    "    elif(activation_function=='sigmoid'):\n",
    "        \n",
    "        diff_act=sigmoid_diff\n",
    "        return(diff_act)\n",
    "    \n",
    "    elif(activation_function=='linear'):\n",
    "        \n",
    "        diff_act=linear_diff\n",
    "        return(diff_act)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print(\"\\nYou entered wrong activation function\\n\")\n",
    "        print(\"**Please Start Again**\")\n",
    "        main_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_j(data_point,V,j,input_layer):\n",
    "    \n",
    "    sum_j=0\n",
    "    for m in range(input_layer):\n",
    "            sum_j=(sum_j)+(V[j][m]*(data_point[m]))\n",
    "    return(sum_j)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def values_hj(output_active,netj):\n",
    "    \n",
    "    return(output_active(netj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_k(hj,W,k,hidden_layer):\n",
    "    \n",
    "    sum_k=0\n",
    "    for j in range(hidden_layer):\n",
    "        sum_k=sum_k+(W[k][j]*(hj[j]))\n",
    "    return(sum_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_kj_reg(W,err,hj,LR):\n",
    "    \n",
    "    new_updated=W+(LR*(err)*hj)\n",
    "    return(new_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ji_reg(V,err,data_point,LR,netj,W,m,diff_active):\n",
    "    \n",
    "    ji_updated=(V)+(LR*err*W*diff_active(netj)*data_point[m])\n",
    "    return(ji_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression using Batch Gradient descent\n",
    "def regression_layer_3(layers):\n",
    "    \n",
    "    output_layer=1        \n",
    "    \n",
    "    input_layer=input_layer_nodes(training_data,\"NO\")  \n",
    "    \n",
    "    hidden_layer=int(input(\"Enter the number of nodes you want to insert in the Hidden Layer\"))\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    hidden_activation=select_activation(layers)\n",
    "    \n",
    "    output_active=hidden_activation[0]\n",
    "    \n",
    "    diff_active=hidden_activation[1]\n",
    "    \n",
    "    for l in range(5):\n",
    "\n",
    "            \n",
    "        #There are different ways to initialize the weights\n",
    "        #1) Some random initialization\n",
    "        #2) Most widely used Xaviers Initialization\n",
    "\n",
    "        V= [[0 for x in range(input_layer)] for y in range(hidden_layer)]  #Initialize the weights from input layer to the hidden layer\n",
    "        \n",
    "        for j in range(hidden_layer):\n",
    "            \n",
    "            for m in range(input_layer):\n",
    "                \n",
    "                V[j][m]=np.random.uniform(-0.00001,0.00001)\n",
    "                    \n",
    "        W=[[0 for x in range(hidden_layer)] for y in range(output_layer)] #Intialize the weights from Hidden Layer to the output layer\n",
    "        \n",
    "        for k in range(output_layer):\n",
    "            \n",
    "            for j in range(hidden_layer):\n",
    "                \n",
    "                W[k][j]=np.random.uniform(-0.00001,0.00001)\n",
    "                    \n",
    "\n",
    "        LR=(1/(10**(l+1)))\n",
    "    \n",
    "        print(\"Round of LR\",(l+1))\n",
    "        \n",
    "        print(LR)\n",
    "\n",
    "\n",
    "        for number in range(1000):\n",
    "            \n",
    "            print(number)\n",
    "                \n",
    "            error=0 \n",
    "            err=0\n",
    "            for i in range(total_length):\n",
    "                    \n",
    "                netj=[0 for m in range(hidden_layer)] \n",
    "                \n",
    "                for j in range(hidden_layer):\n",
    "                    \n",
    "                    netj[j]=net_j(training_data.iloc[i],V,j,input_layer)\n",
    "                \n",
    "                hj=[0 for n in range(hidden_layer)]\n",
    "                \n",
    "                for j in range(hidden_layer):\n",
    "                    \n",
    "                    hj[j]=values_hj(output_active, netj[j])\n",
    "            \n",
    "                netk=[0 for i in range(output_layer)]\n",
    "                \n",
    "                for k in range(output_layer):\n",
    "                    \n",
    "                    netk[k]=net_k(hj,W,k,hidden_layer)\n",
    "                            \n",
    "                \n",
    "                for k in range(output_layer):\n",
    "                    \n",
    "                    predicted = netk[k]\n",
    "                    \n",
    "                    err =(y[i]-predicted) \n",
    "                    \n",
    "                    error=error+((y[i]-predicted)*(y[i]-predicted))\n",
    "    \n",
    "                for k in range(output_layer):\n",
    "                    \n",
    "                    for j in range(hidden_layer):\n",
    "                        \n",
    "                        W[k][j]=update_kj_reg(W[k][j],err,hj[j],LR)\n",
    "                                \n",
    "                                \n",
    "  \n",
    "                for j in range(hidden_layer):\n",
    "                    \n",
    "                    for m in range(input_layer):\n",
    "                        \n",
    "                        for k in range(output_layer):\n",
    "                            \n",
    "                            V[j][m]=update_ji_reg(V[j][m],err,training_data.iloc[i],LR,netj[j],W[k][j],m,diff_active)\n",
    "   \n",
    "       \n",
    "            print(\"Total Error:\",error/(len(training_data)))\n",
    "        \n",
    "        print(\"The Weights Wkj\",W)\n",
    "        \n",
    "        print(\"The Weights Vji\",V)\n",
    "        \n",
    "        print(\"\\n\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression using Stochastic Gradient descent\n",
    "def regression_layer_3(layers):\n",
    "    \n",
    "    output_layer=1                                                          \n",
    "    \n",
    "    input_layer=input_layer_nodes(training_data,\"NO\")  \n",
    "    \n",
    "    hidden_layer=int(input(\"Enter the number of nodes you want to insert in the Hidden Layer\"))\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    hidden_activation=select_activation(layers)\n",
    "    \n",
    "    output_active=hidden_activation[0]\n",
    "    \n",
    "    diff_active=hidden_activation[1]\n",
    "    \n",
    "    inner_iterations=int(imput(\"Enter number of times you want to perform iterations on each data point\"))\n",
    "    \n",
    "    for l in range(5):\n",
    "\n",
    "            \n",
    "        #There are different ways to initialize the weights\n",
    "        #1) Some random initialization\n",
    "        #2) Most widely used Xaviers Initialization\n",
    "        V= [[0 for x in range(input_layer)] for y in range(hidden_layer)]  #Initialize the weights from input layer to the hidden layer\n",
    "        \n",
    "        for j in range(hidden_layer):\n",
    "            \n",
    "            for m in range(input_layer):\n",
    "                \n",
    "                V[j][m]=np.random.uniform(-0.00001,0.00001)\n",
    "        \n",
    "        W=[[0 for x in range(hidden_layer)] for y in range(output_layer)] #Intialize the weights from Hidden Layer to the output layer\n",
    "        \n",
    "        for k in range(output_layer):\n",
    "            \n",
    "            for j in range(hidden_layer):\n",
    "                \n",
    "                W[k][j]=np.random.uniform(-0.00001,0.00001)\n",
    "                    \n",
    "\n",
    "        LR=(1/(10**(l+1)))\n",
    "    \n",
    "        print(\"Round of LR\",(l+1))\n",
    "        \n",
    "        print(LR)\n",
    "  \n",
    "        for i in range(total_length):#Length of the data set\n",
    "                \n",
    "            error=0 \n",
    "            err=0\n",
    "                \n",
    "            for p in range(inner_iterations):#In Stochastic Gradient descent we will cnsider each data point for a fixed number of\n",
    "                                                                     #iterations\n",
    "                    \n",
    "                netj=[0 for m in range(hidden_layer)] \n",
    "                \n",
    "                for j in range(hidden_layer):\n",
    "                    \n",
    "                    netj[j]=net_j(training_data.iloc[i],V,j,input_layer)\n",
    "                        \n",
    "            \n",
    "                hj=[0 for n in range(hidden_layer)]\n",
    "                \n",
    "                for j in range(hidden_layer):\n",
    "                    \n",
    "                    hj[j]=values_hj(output_active, netj[j])\n",
    "            \n",
    "                netk=[0 for i in range(output_layer)]\n",
    "                \n",
    "                for k in range(output_layer):\n",
    "                    \n",
    "                    netk[k]=net_k(hj,W,k,hidden_layer)\n",
    "                            \n",
    "                \n",
    "                for k in range(output_layer):\n",
    "                    \n",
    "                    predicted = netk[k]\n",
    "                    \n",
    "                    err =(y[i]-predicted) \n",
    "                    \n",
    "                    error=error+((y[i]-predicted)*(y[i]-predicted))\n",
    "            \n",
    "                \n",
    "\n",
    "                for k in range(output_layer):\n",
    "                    \n",
    "                    for j in range(hidden_layer):\n",
    "                        \n",
    "                        W[k][j]=update_kj_reg(W[k][j],err,hj[j],LR)\n",
    "                                \n",
    "                                 \n",
    "                                \n",
    "  \n",
    "                for j in range(hidden_layer):\n",
    "                    \n",
    "                    for m in range(input_layer):\n",
    "                        \n",
    "                        for k in range(output_layer):\n",
    "                             \n",
    "                            V[j][m]=update_ji_reg(V[j][m],err,training_data.iloc[i],LR,netj[j],W[k][j],m,diff_active)\n",
    "   \n",
    "       \n",
    "        print(\"Total Error:\",error/(len(training_data)))\n",
    "        \n",
    "        print(\"The Weights Wkj\",W)\n",
    "        \n",
    "        print(\"The Weights Vji\",V)\n",
    "        \n",
    "        print(\"\\n\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mini Batch Gradient descent\n",
    "#The main disadvanatge of the Batch Gradient descent is that we may get struck in the local minima(saddle point)\n",
    "#The stochastic gradient descebt may help us in not getting struck in sadle point but the computational time is very high\n",
    "#To avoid both these strucking in local minima and Time complexity we will use this technique called MINI BATCH GRADIENT DESCENT\n",
    "def regression_layer_3(layers):\n",
    "    \n",
    "    output_layer=1                                                          \n",
    "    \n",
    "    input_layer=input_layer_nodes(training_data,\"NO\")  \n",
    "    \n",
    "    hidden_layer=int(input(\"Enter the number of nodes you want to insert in the Hidden Layer\"))\n",
    "    \n",
    "    hidden_activation=select_activation(layers)\n",
    "    \n",
    "    output_active=hidden_activation[0]\n",
    "    \n",
    "    diff_active=hidden_activation[1]\n",
    "    \n",
    "    batch=int(input('Enter the number of batches you want to divide the data'))\n",
    "    \n",
    "    train_batch=[0 for i in range(batch)]\n",
    "    \n",
    "    a=0\n",
    "    \n",
    "    for i in range(batch):\n",
    "        \n",
    "        b=(((len(training_data))/batch)*(i+1))-1\n",
    "        \n",
    "        train_batch[i]=training_data[a:b]\n",
    "        \n",
    "        a=b\n",
    "    \n",
    "    for l in range(5):\n",
    "\n",
    "            \n",
    "        #There are different ways to initialize the weights\n",
    "        #1) Some random initialization\n",
    "        #2) Most widely used Xaviers Initialization\n",
    "\n",
    "        V= [[0 for x in range(input_layer)] for y in range(hidden_layer)]  #Initialize the weights from input layer to the hidden layer\n",
    "        \n",
    "        for j in range(hidden_layer):\n",
    "            \n",
    "            for m in range(input_layer):\n",
    "                \n",
    "                V[j][m]=np.random.uniform(-0.00001,0.00001)\n",
    "                    \n",
    "        W=[[0 for x in range(hidden_layer)] for y in range(output_layer)] #Intialize the weights from Hidden Layer to the output layer\n",
    "        \n",
    "        for k in range(output_layer):\n",
    "            \n",
    "            for j in range(hidden_layer):\n",
    "                \n",
    "                W[k][j]=np.random.uniform(-0.00001,0.00001)\n",
    "            \n",
    "        LR=(1/(10**(l+1)))\n",
    "\n",
    "        print(\"Round of LR\",(l+1))\n",
    "\n",
    "        print(LR)\n",
    "    \n",
    "        for j in range(batch):\n",
    "            \n",
    "            training_data=train_batch[j]\n",
    "            \n",
    "            for number in range(1000):\n",
    "                \n",
    "                error=0 \n",
    "                \n",
    "                err=0\n",
    "                    \n",
    "                for i in range(len(training_data)):\n",
    "                        \n",
    "                    netj=[0 for m in range(hidden_layer)] \n",
    "                    \n",
    "                    for j in range(hidden_layer):\n",
    "                        \n",
    "                        netj[j]=net_j(training_data.iloc[i],V,j,input_layer)\n",
    "                        \n",
    "            \n",
    "                    hj=[0 for n in range(hidden_layer)]\n",
    "                    \n",
    "                    for j in range(hidden_layer):\n",
    "                        \n",
    "                        hj[j]=values_hj(output_active, netj[j])\n",
    "            \n",
    "                    netk=[0 for i in range(output_layer)]\n",
    "                    \n",
    "                    for k in range(output_layer):\n",
    "                        \n",
    "                        netk[k]=net_k(hj,W,k,hidden_layer)\n",
    "                            \n",
    "                \n",
    "                    for k in range(output_layer):\n",
    "                        \n",
    "                        predicted = netk[k]\n",
    "                        \n",
    "                        err =(y[i]-predicted) \n",
    "                        \n",
    "                        error=error+((y[i]-predicted)*(y[i]-predicted))\n",
    "            \n",
    "                \n",
    "\n",
    "                    for k in range(output_layer):\n",
    "                        \n",
    "                        for j in range(hidden_layer):\n",
    "                            \n",
    "                            W[k][j]=update_kj_reg(W[k][j],err,hj[j],LR)\n",
    "                                    \n",
    "                                    \n",
    "  \n",
    "                    for j in range(hidden_layer):\n",
    "                        \n",
    "                        for m in range(input_layer):\n",
    "                            \n",
    "                            for k in range(output_layer):\n",
    "                                \n",
    "                                V[j][m]=update_ji_reg(V[j][m],err,training_data.iloc[i],LR,netj[j],W[k][j],m,diff_active)\n",
    "        \n",
    "       \n",
    "            print(\"Total Error:\",error/(len(training_data)))\n",
    "        \n",
    "        print(\"The Weights Wkj\",W)\n",
    "        \n",
    "        print(\"The Weights Vji\",V)\n",
    "        \n",
    "        print(\"\\n\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This is the main Regression Function\n",
    "def regression_main(Number_of_Layers):\n",
    "    print(\"\\n The loss for the regression is Root mean Squared Loss\\n\")\n",
    "    layers=Number_of_Layers\n",
    "    if(layers==3):\n",
    "        regression_layer_3(layers)\n",
    "    else:\n",
    "        print(\"\\nThe number of layers you wnat to insert is completely incorrect\")\n",
    "        print(\"\\n**Please Try again**\")\n",
    "        main_function()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_function():\n",
    "    print(\"Purpose is Regression \\n\")\n",
    "    print(\"As of now the Number of Layers in neural Network is 3\\n\")\n",
    "#The entered input is always considered as string and to change that we need to convert that into \n",
    "    Purpose_of_Neural_Network = str(input(\"Enter the purpose of your Neural network:\")) \n",
    "    Number_of_Layers=int(input(\"Enter the number of Layers you want in your Neural Network:\"))\n",
    "    if(Purpose_of_Neural_Network == \"Regression\" ):\n",
    "            regression_main(Number_of_Layers)\n",
    "    else:\n",
    "        print(\"Entered Choice Does Not Exsist\\n\")\n",
    "        print(\"**Please Start Again**\")\n",
    "        main_function()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is by using the Scikit_learn Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using scikit learn library\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(training_data, y)# Finding the parameters\n",
    "lin_reg.intercept_, lin_reg.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions=lin_reg.predict(training_data)\n",
    "print(\"\\n The MSE Error for testing is:\",mean_squared_error(y,train_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
